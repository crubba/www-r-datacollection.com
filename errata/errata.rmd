---
output:
  pdf_document: default
  html_document:
    highlight: pygments
    keep_md: yes
    theme: readable
---
# Errata for 'Automated Data Collection with R'


Last update: `r Sys.time()`

```{r, label = 'setup', message = F, results = 'hide'}
library(stringr)
library(RCurl)
library(XML)
library(rvest)
```


# page 2

*Credit: Suryapratim Sarkar (2015-06-25)*

Wikipedia changed its server communication from HTTP to HTTPS. As a result, the following lines on page 2 return an error:

```{r, label = 'p2', cache = TRUE, eval = T, error = T}
heritage_parsed <- htmlParse("http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger", 
                             encoding = "UTF-8")
```

There are at least two solutions to the problem:

1. Use `getURL()` and specify the location of CA signatures (see Section 9.1.7 of our book).

2. Use Hadley Wickham's `rvest` package, which came out after our book was published. It facilitates scraping with R considerably, in particular in such scenarios. In this specific example, use the following code instead:

```{r, , label = 'p2a', cache = TRUE, eval = F}
library(rvest) # the new package, version 0.3.0
heritage_parsed <- read_html("http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger", encoding = "UTF-8") # read_html() from the rvest package is the new htmlParse() from the XML package
tables <- html_table(heritage_parsed, fill = TRUE) # html_table() from the rvest package, which replaces readHTMLTable() from the XML package
```

From thereon, the rest of the chapter code should work. If you want to learn more about the rvest package, have a look [here](http://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/). We are planning to cover it extensively in the next edition of our book.




# page 35

*Credit: JÃ¼ri Kuusik (2015-01-22)*

typo: change *"parsed_doc"* to *"parsed_fortunes$children"* 



# page 136

*Credit: Laurent Franckx (2015-02-18)*

Due to (supposedly) a bug in the RCurl package (version 1.95-4.5, bug has been reported) the following lines on page 136 give an error:

```{r, , label = 'p136', include = FALSE, cache = TRUE, eval = F}
require(RCurl)
require(stringr)
url <- "http://www.r-datacollection.com/materials/http/helloworld.html"
```

```{r, label = 'p136b', error = TRUE, cache = TRUE, eval = F}
handle <- getCurlHandle(customrequest = "HEAD")
res <- getURL(url = url, curl = handle, header = TRUE)
```


There are two workaronds at the moment. 

(1) To make the code simply run through you might give up the HTTP HEAD method used in the code above and use `customrequest = "GET"` instead:

```{r, label = 'p136c', error = TRUE, cache = TRUE, eval = F}
require(RCurl)
require(stringr)

url <- "http://www.r-datacollection.com/materials/http/helloworld.html"
res <- getURL(url = url, header = TRUE)
cat(str_split(res, "\r")[[1]])

handle <- getCurlHandle(customrequest = "GET")
res <- getURL(url = url, curl = handle, header = TRUE)
```


(2) If you want to have a working example involving HTTP HEAD method you might switch to the httr package like this:

```{r, label = 'p136d', cache = TRUE}
require(httr)

url <- "http://www.r-datacollection.com/materials/http/helloworld.html"
res <- HEAD(url)

res
res$request
res$headers[1:3]
```


# page 194

*Reported by: Laurent Franckx (2015-05-11)*

The URL on page 194 to the parlgov SQLite database has changed and does not work anymore. The new URL is:

http://www.parlgov.org/static/stable/2014/parlgov-stable.db


# page 249

*Reported by: Laurent Franckx (2015-06-08)*

The page structure had changed and code did not work anymore. 

```{r, label = 'p249', include=FALSE}
# preparations
info   <- debugGatherer()
handle <- getCurlHandle(cookiejar      = "", 
                           followlocation = TRUE, 
                           autoreferer    = TRUE,
                           debugfunc      = info$update,
                           verbose        = TRUE,
                           httpheader     = list(
                             from         = "eddie@r-datacollection.com",
                             'user-agent' = str_c(R.version$version.string, 
                                              ", ", R.version$platform)
                           ))
```


```{r, label = 'p249b', cache=TRUE}
# define urls						   
search_url <- "www.biblio.com/search.php?keyisbn=data"
cart_url   <- "www.biblio.com/cart.php"

# download and parse page
search_page <- htmlParse(getURL(url = search_url, curl = handle))

# identify form fields
xpathApply(search_page, "//div[@class='row-fixed'][position()<2]/form")

# extract book ids
xpath <- "//div[@class='row-fixed'][position()<4]/form/input[@name='bid']/@value"
bids <- unlist(xpathApply(search_page, xpath, as.numeric))
bids

# add items to shopping cart
for(i in seq_along(bids))  {
	res <- getForm(uri = cart_url, 
	                 curl = handle, 
	                 bid = bids[i], 
	                 add = 1, 
	                 int = "keyword_search")
}

# inspect shopping cart
cart  <- htmlParse(getURL(url=cart_url, curl=handle))
clean <- function(x)  str_replace_all(xmlValue(x),"(\t)|(\n)"," ")
xpathSApply(cart, "//h3/a", clean)

# request header
cat(str_split(info$value()["headerOut"],"\r")[[1]][1:13])

# response header
cat(str_split(info$value()["headerIn"],"\r")[[1]][1:14])

```

# page 254

*Reported by: Laurent Franckx (2015-06-10)*

There has been a change to the install_github function of the devtools package. To install Rwebdriver use:

```{r, label = 'p254', cache = TRUE, eval = F}
library(devtools)

install_github("crubba/Rwebdriver")
````


# page 299--310

The website holding the UK government press releases has been altered slightly. To get the date and organisation you need to change the XPaths here...

```{r, cache = TRUE, eval = F}
library(XML)

organisation <- xpathSApply(tmp, "//dl[@data-trackposition='top']//a[@class='organisation-link']", xmlValue)
publication <- xpathSApply(tmp, "//dl[@class='primary-metadata']//abbr[@class='date']", xmlValue)
````

... and here...

```{r, cache = TRUE, eval = F}
for(i in 2:length(list.files("Press_Releases/"))){
    tmp <- readLines(str_c("Press_Releases/", i, ".html"))
    tmp <- str_c(tmp, collapse = "")
    tmp <- htmlParse(tmp)
    release <- xpathSApply(tmp, "//div[@class='block-4']", xmlValue)
    organisation <- xpathSApply(tmp, "//dl[@data-trackposition='top']//a[@class='organisation-link']", xmlValue)
    publication <- xpathSApply(tmp, "//dl[@class='primary-metadata']//abbr[@class='date']", xmlValue)
    if(length(release) != 0){
        n <- n + 1
        tmp_corpus <- Corpus(VectorSource(release))
        release_corpus <- c(release_corpus, tmp_corpus)
        meta(release_corpus[[n]], "organisation") <- organisation[1]
        meta(release_corpus[[n]], "publication") <- publication
    }
}
```

The prescindMeta() function is defunct as of version 0.6 of the tm package. The meta data can now be gathered with the meta() function.

```{r, cache = TRUE, eval = F}
meta_organisation <- meta(release_corpus, type = "local", tag = "organisation")
meta_publication <- meta(release_corpus, type = "local", tag = "publication")

meta_data <- data.frame(
    organisation = unlist(meta_organisation), 
    publication = unlist(meta_publication)
)
```

The sFilter() function is also defunct. You can filter the corpus using meta().

```{r, cache = TRUE, eval = F}
release_corpus <- release_corpus[
    meta(release_corpus, tag = "organisation") == "Department for Business, Innovation & Skills" |
    meta(release_corpus, tag = "organisation") == "Department for Communities and Local Government" |
    meta(release_corpus, tag = "organisation") == "Department for Environment, Food & Rural Affairs" |
    meta(release_corpus, tag = "organisation") == "Foreign & Commonwealth Office" |
    meta(release_corpus, tag = "organisation") == "Ministry of Defence" |
    meta(release_corpus, tag = "organisation") == "Wales Office"        
]
```

The *stringr* package also produces a hick-up with the updated version of the tm package, thus we switch to base R.

```{r, cache = TRUE, eval = F}
tm_filter(release_corpus, FUN = function(x) any(grep("Afghanistan", content(x))))
```

We need to wrap the replace function with the new content_transformer()...

```{r, cache = TRUE, eval = F}
release_corpus <- tm_map(
    release_corpus, 
    content_transformer(
        function(x, pattern){
            gsub(
                pattern = "[[:punct:]]", 
                replacement = " ",
                x
            )
        )
    )
)
```

Moreover, the tolower() function needs to be wrapped with the content_transformer()...

```{r, cache = TRUE, eval = F}
release_corpus <- tm_map(release_corpus, content_transformer(tolower))
```

The prescindMeta() function is also defunct on page 310...

```{r, cache = TRUE, eval = F}
org_labels <- unlist(meta(release_corpus, "organisation"))
```

# page 315

Since the sFilter() and prescindMeta() functions are defunct as of version 0.6 of the tm package, you need to change the code on page 315 to filter the corpus.

```{r, cache = TRUE, eval = F}
short_corpus <- release_corpus[c(
    which(
        meta(
            release_corpus, tag = "organisation"
        ) == "Department for Business, Innovation & Skills"
    )[1:20],
    which(
        meta(
            release_corpus, tag = "organisation"
        ) == "Wales Office"
    )[1:20],
    which(
        meta(
            release_corpus, tag = "organisation"
        ) == "Department for Environment, Food & Rural Affairs"
    )[1:20]
)]

table(unlist(meta(short_corpus, "organisation")))
```




# page 243 / 9.1.5.3

*reported by Jane Yu*

The installation of RHTMLForms package would fail - it's omgehat.net not omegahat.org. Use this instead ...

```{r, eval=FALSE}
install.packages("RHTMLForms", repos = "http://www.omegahat.net/R", type="source")
```






